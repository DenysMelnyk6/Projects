{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "pd.set_option('display.max_columns', 200) # show more columns\n",
    "RANDOM_SEED = 27 # makes the test repeatable\n",
    "current_date = pd.to_datetime('2021/03/15') # current date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets contains information about restaurants. This infomation needs to be used to build a regression model capable of predicting the rating of a restaurant. \n",
    "First data frame, df_train(main_task.csv) contains information about 40000 restaurants and will be used for training and validating the model.\n",
    "The df_test('kaggle_task.csv) has information about 10000 restaurants, which rating needs to be predicted.\n",
    "Following information can be found in the data frames:\n",
    "1. Restaurant_id - restaurant identification number;\n",
    "2. City - city in which restaurant is located;\n",
    "3. Cuisine Style - tags for food that is served in the restaurant;\n",
    "4. Ranking - rank of a restaurant compared to the other restaurants in the city;\n",
    "5. Rating -  restaurant rating according to TripAdvisor, dependant variable and the value that needs to be predicted(1 to 5);\n",
    "6. Price Range - range of prices in a restaurant(category);\n",
    "7. Number of Reviews — Number of Reviews;\n",
    "8. Reviews - two reviews displayed on the website and their respective dates;\n",
    "9. URL_TA — URL on TripAdvisor;\n",
    "10. ID_TA — Identificator of restaurant in TripAdvisor's DataBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /kaggle/input/sf-dst-restaurant-rating//main_task.csv does not exist: '/kaggle/input/sf-dst-restaurant-rating//main_task.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-67db930e694c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/kaggle/input/sf-dst-restaurant-rating/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/main_task.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# dataset for training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'kaggle_task.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# dataset for validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msample_submission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/sample_submission.csv'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# submission dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File /kaggle/input/sf-dst-restaurant-rating//main_task.csv does not exist: '/kaggle/input/sf-dst-restaurant-rating//main_task.csv'"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\n",
    "df_train = pd.read_csv('main_task.csv') # dataset for training\n",
    "df_test = pd.read_csv('kaggle_task.csv') # dataset for validation\n",
    "sample_submission = pd.read_csv('sample_submission.csv') # submission dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the correct processing of features, combine train and test sets into a one dataset\n",
    "\n",
    "df_train['sample'] = 1  # train\n",
    "df_test['sample'] = 0  # test\n",
    "# as we have to predict rating, in test set we just fill it with 0\n",
    "df_test['Rating'] = 0\n",
    "\n",
    "df = df_test.append(df_train, sort=False).reset_index(\n",
    "    drop=True)  # combine sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Basic look at the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns so they are easier to read\n",
    "df.columns = ['id','city', 'cuisine', 'ranking',\n",
    "              'prices', 'number_of_reviews', 'reviews', 'url',\n",
    "              'website_id', 'sample', 'rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the moment url is not used, so it is dropped at the start\n",
    "df.drop(['url'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general look at the number of unique values in df\n",
    "df.nunique(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the column id has repeatable values, which might be duplicates.\n",
    "The columns cuisine, prices and number_of_reviews have 9283, 13886 and 2543 NaN values.\n",
    "Most columns have data dype object, with only ranking, rating and number_of_reviews being float64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Restaurant ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.id = df.id.apply(lambda x: x[3:]) # getting rid of the id_ part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.id == df.id.value_counts().index[0]] # displaying the restaurants that share id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the line are not duplicated, so the restaurants sharing id are just chain restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marking such restaurants should we need it in the future\n",
    "chain_restaurant = df.id.value_counts()[df.id.value_counts() > 1].index.tolist()\n",
    "df['chain_restaurant'] = df[df.id.isin(chain_restaurant)].id.apply(lambda x: 1)\n",
    "df.chain_restaurant = df['chain_restaurant'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any repetitions/alternative notations for city names\n",
    "df.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "sns.countplot(df['city'], order=df['city'].value_counts().index)\n",
    "plt.title('Cities Distribution\\n', fontsize=15)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('City')\n",
    "\n",
    "cities = df.city.unique() # creating list with all the cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cuisine[0], type(df.cuisine[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that data in this column is a string, arrenged to look like a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation(data):\n",
    "    '''Function for separation of certain data in the dataset.\n",
    "       Function takes in a string and returns a\n",
    "       list of elements of the string'''\n",
    "    data = data.replace('[','')\n",
    "    data = data.replace(']','')\n",
    "    data = data.replace('\\'','')\n",
    "    data = data.lower()\n",
    "    data = data.split(',')\n",
    "    data = [element.strip() for element in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marking the lines that had their values filled in\n",
    "df['filled_cuisine'] = df[df.cuisine.isna()].cuisine.apply(lambda x: 1)\n",
    "df['filled_cuisine'] = df['filled_cuisine'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the empty lines and separates the strings, making lists of tags of cuisines\n",
    "df.cuisine = df.cuisine.fillna('other_cuisine').apply(separation)\n",
    "cuisine_exploded = df.explode('cuisine')\n",
    "pd.DataFrame(cuisine_exploded.cuisine.value_counts()) # counts down how often each cousine is seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary with most common cuisine in each city\n",
    "popular_cuisine_dict = {}\n",
    "for city in cities:\n",
    "    most_popular_cuisine = cuisine_exploded[\n",
    "        cuisine_exploded['city'] == city\n",
    "    ].cuisine.value_counts().index[0]\n",
    "    popular_cuisine_dict[city] = most_popular_cuisine\n",
    "popular_cuisine_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps dictionary with common cuisine on the dataframe\n",
    "df['most_popular_cuisine'] = df.city.map(popular_cuisine_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks how many cuisine styles a restaurant offers. \n",
    "# Since initially the tags weren't included for some restaurants, their values of number\n",
    "# of tags are set to 0\n",
    "df['cuisine_count'] = df.cuisine.apply(lambda x: len(x))\n",
    "df['cuisine_count'].loc[df.filled_cuisine == 1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "sns.boxplot(df.ranking.values)\n",
    "plt.title('Ranking boxplot\\n', fontsize = 15) # boxplot to see the distribution of values of ranking\n",
    "plt.xlabel('Ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.distplot(df.ranking.values, bins=25) # How frequent the ranks are\n",
    "plt.title('Ranking distribution\\n', fontsize = 15)\n",
    "plt.xlabel('Ranking')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ranking'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph, it is seen that distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for city in (df['city'].value_counts())[0:10].index:\n",
    "    sns.distplot(df['ranking'][df['city'] == city], kde=False, label=city)\n",
    "\n",
    "    \n",
    "plt.legend(prop={'size': 10})\n",
    "plt.title('Ranking Distribution among cities\\n', fontsize=15)\n",
    "plt.xlabel('Ranking')\n",
    "plt.ylabel('Quantity (frequency)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above it is clear that all the cities use the same distribution, so ranking can be normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ranking_norm'] = df['ranking']/df.city.map(dict(df['city'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at the distribution of normalised ranking\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.distplot(df.ranking_norm.values, bins=25)\n",
    "plt.title('Ranking Distribution\\n', fontsize=15)\n",
    "plt.xlabel('Ranking')\n",
    "plt.ylabel('Quantity (frequency)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ranking_norm'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df.ranking_norm.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Reviews and number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "sns.boxplot(df.number_of_reviews.values)\n",
    "df.number_of_reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reviews[0], type(df.reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with cuisine, the reviews column appears to be a string mimicing a list. \n",
    "Also, the date of the reviews is attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a template for search\n",
    "lrx = re.compile('\\[\\[.*\\]\\]')\n",
    "nan = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_extraction(row):\n",
    "    '''Function is called for extracting data from column \n",
    "    reviews and splitting it out into a separate columns\n",
    "    INPUT: Whole dataset\n",
    "    OUTPUT: Dataset with additional columns'''\n",
    "\n",
    "    cell = row['reviews']\n",
    "    aux_list = [[], []]  # create an auxilliary list for saving temp.data\n",
    "    if type(cell) == str and lrx.fullmatch(cell):  # compare with searech template\n",
    "        aux_list = eval(cell)  # transform into a list\n",
    "        \n",
    "    \n",
    "    row['first_date'] = pd.to_datetime(\n",
    "    aux_list[1][1] if len(aux_list[1]) > 1 else nan)\n",
    "    row['last_date'] = pd.to_datetime(aux_list[1][0] if len(\n",
    "        aux_list[1]) > 0 else nan, format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "    row['first_date'] = pd.to_datetime(row['first_date'])\n",
    "    row['last_date'] = pd.to_datetime(row['last_date'])\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "df = df.apply(date_extraction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a diffderence between date of the first review and the last one\n",
    "# add this information into a new column\n",
    "\n",
    "df['time_difference'] = df['last_date'] - df['first_date']\n",
    "\n",
    "# call the function and get difference in days\n",
    "df['time_difference'] = df['time_difference'].apply(\n",
    "    lambda x: abs(x.days)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['time_difference'].describe())\n",
    "sns.boxplot(df['time_difference'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that most reviews have been left over the span of the last 4 months or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days_since_last_review'] = current_date - df['last_date']\n",
    "df['days_since_last_review'] = df['days_since_last_review'].apply(\n",
    "    lambda x: x.days\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['days_since_last_review'].describe())\n",
    "sns.boxplot(df['days_since_last_review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marking the lines that had their values filled in\n",
    "df['last_date_filled'] = df[df.last_date.isna()].last_date.apply(lambda x: 1)\n",
    "df['last_date_filled'] = df['last_date_filled'].fillna(0)\n",
    "df['first_date_filled'] = df[df.first_date.isna()].first_date.apply(lambda x: 1)\n",
    "df['first_date_filled'] = df['first_date_filled'].fillna(0)\n",
    " \n",
    "df['last_date'] = df['last_date'].fillna(0)\n",
    "df['first_date'] = df['first_date'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_of_reviews'] = df.apply(\n",
    "    lambda row: 1 if np.isnan(row['number_of_reviews']) and\n",
    "    (row['last_date'] == 0 or row['first_date'] == 0)\n",
    "    else row['number_of_reviews'], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_of_reviews'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days_since_last_review'] = df['days_since_last_review'].fillna(0)\n",
    "df['time_difference'] = df['time_difference'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.6 Price Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prices'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price range is hard to read using the original notation, so\n",
    "# it is converted into categorical data\n",
    "def price_range_sort(price):\n",
    "    '''The function takes in a string and replaces it with another string'''\n",
    "    if price == '$$ - $$$':\n",
    "        price = 'medium'\n",
    "    elif price == '$':\n",
    "        price = 'budget'\n",
    "    elif price == '$$$$':\n",
    "        price = 'expensive'\n",
    "    return price\n",
    "\n",
    "\n",
    "df['filled_budget'] = df[df.prices.isna()].prices.apply(lambda x: 1) # marking the filled values\n",
    "df['filled_budget'] = df['filled_budget'].fillna(0)\n",
    "df.prices = df.prices.fillna('unspecified_budget').apply(price_range_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(x = 'prices', y = 'rating',\n",
    "           data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(x = 'prices', y = 'number_of_reviews',\n",
    "           data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 Website ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a few restaurants have a shared website id\n",
    "shared_web = df.website_id.value_counts()[df.website_id.value_counts() > 1].index.to_list()\n",
    "df['shared_web'] = df[df.website_id.isin(shared_web)].website_id.apply(lambda x : 1)\n",
    "df['shared_web'] = df['shared_web'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.8 Dependant Variable - Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['sample'] == 1].rating.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no empty values for the rating, create a graph to see the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.distplot(df.loc[df['sample'] == 1].rating.values, bins=25)\n",
    "plt.title('Rating Distribution\\n', fontsize=15)\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Quantity (frequency)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df.loc[df['sample'] == 1].rating.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of rating are mostly in range from 3.5 to 4.5, with anything below 2 being an outlier. It seems that most restaurants have a rating above 3, which should be an average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the data in this column is not continuous. Let's check it by using unique():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rating.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is not continuous, and rating takes the values which are multiples of 0.5. This can be used to increase the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 New Data\n",
    "To increase the accuracy of the model, more data added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data adds the popultion(in millions of people) and area(km squared)\n",
    "data = [[2.2, 105], [1, 188], [8.9, 1706],\n",
    "         [3.6, 891], [1.5, 310], [0.3, 41.4], \n",
    "         [1.4, 181], [0.4, 368], [1.9, 414], \n",
    "         [2.9, 1287], [1.7, 101], [3.3, 607], \n",
    "         [1.2, 117], [0.2, 32.6], [0.4, 88], \n",
    "         [1.8, 517], [1.8, 525], [0.8, 86.4], \n",
    "         [0.9, 219], [0.5, 48], [1.9, 755], \n",
    "         [0.5,100], [1.3, 496], [0.7, 454], \n",
    "         [0.7,213], [0.5, 120], [0.2, 16], \n",
    "         [0.3, 163], [0.7, 39], [0.125, 51.5], \n",
    "         [0.8, 327]]\n",
    "city_dict = {}\n",
    "for i in range(len(cities)):\n",
    "    city_dict[cities[i]] = data[i] \n",
    "city_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds the data to the data frame\n",
    "df['population_and_area'] = df.city.map(city_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new variable of population density. It appears that keping area and population have a positive impact on MAE, so they are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the data from the list, calculates the populaion density\n",
    "df['population'] = df['population_and_area'].apply(lambda x: x[0])\n",
    "df['area'] = df['population_and_area'].apply(lambda x: x[1])\n",
    "df['population_density'] = df['population_and_area'].apply(lambda x: x[0]/x[1])\n",
    "df.drop(['population_and_area'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Capitals\n",
    "It is possible that restaurants in the capital might have their rating inflated, or they might be better due to\n",
    "larger number of tourists. to check that, a new column will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital = [1, 1, 1, 0, 1, 0, 1, \n",
    "           1, 1, 1, 1, 1, 0, 0, \n",
    "           0, 1, 1, 1, 1, 1, 1, \n",
    "           1, 0, 0, 0, 0, 1, \n",
    "           1, 1, 1, 1]\n",
    "\n",
    "capital_dict = {}\n",
    "for i in range(len(cities)):\n",
    "    capital_dict[cities[i]] = capital[i] \n",
    "capital_dict\n",
    "df['in_capital'] = df.city.map(capital_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital = [1, 1, 1, 1, 0, 0,\n",
    "          0, 1, 1, 1, 0, 1,\n",
    "          1, 1, 1, 1, 1, 1,\n",
    "          1, 0, 0, 1, 1, 1,\n",
    "          1, 0, 0, 1, 1, 1, 0]\n",
    "\n",
    "capital_dict = {}\n",
    "for i in range(len(cities)):\n",
    "    capital_dict[cities[i]] = capital[i] \n",
    "capital_dict\n",
    "df['in_capital'] = df.city.map(capital_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Preparing the dataset for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Correlations\n",
    "Checking the correlations between the columns, to see if any of them need to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df.corr()\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few values have very high or very low correlation. For this model, I have decided to treat the absolute value of the correlation of 0.7 or higher as high. Let's display the rows which have correlations that might be outside the boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations['ranking'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations['cuisine_count'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations['time_difference'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations['population_density'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations['rating'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that certain parameters have a fairly high correlation between them, but none of them were over the treshhold of 0.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Creating dummies\n",
    "Creating dummy variables so the model can use them for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_dummies = pd.get_dummies(df.city)\n",
    "cuisine_dummies = pd.get_dummies(df.cuisine.apply(pd.Series).stack()).sum(level = 0)\n",
    "prices_dummies = pd.get_dummies(df.prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(city_dummies)\n",
    "np.shape(cuisine_dummies)\n",
    "np.shape(prices_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Removing columns from the model dataframe\n",
    "Certain columns can not be used by the model, so they are added to the list 'to_drop' and then dropped.\n",
    "Colums that had negative impact on MAE are added to 'negative_impact' list and then dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df.loc[df['sample'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns that the model can not use for prediction. Columns 'filled_cuisine', 'filled_budget', 'chain_restaurant', 'shared_web', 'last_date_filled', 'first_date_filled' appear to have a negative impact on MAE, therefore are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['id', 'city', 'cuisine', 'prices',\n",
    "           'reviews', 'website_id', 'most_popular_cuisine',\n",
    "          'first_date', 'last_date', 'sample']\n",
    "negative_impact = [ 'filled_cuisine', 'filled_budget',\n",
    "                   'chain_restaurant', 'shared_web',\n",
    "                   'last_date_filled', 'first_date_filled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropping lines are separated so the model can be tested with the columns that have negative impact on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model.drop(to_drop, axis=1)\n",
    "df_model = df_model.drop(negative_impact, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dummies with the dataframe made for the model training\n",
    "df_model = df_model.merge(city_dummies,\n",
    "                          left_index=True, right_index=True)\n",
    "df_model = df_model.merge(cuisine_dummies,\n",
    "                          left_index=True, right_index=True)\n",
    "df_model = df_model.merge(prices_dummies,\n",
    "                          left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(df_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model\n",
    "X - independent variable, data set with the information about the restaurants;\n",
    "y - dependant variable, rating of a restaurant, which the model is trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model.drop(['rating'], axis = 1)\n",
    "y = df_model['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y train are used for training the model\n",
    "# X and y test are used for validation of the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing:\n",
    "from sklearn.ensemble import RandomForestRegressor # for creating and training a model\n",
    "from sklearn import metrics # for assessing model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "regr = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# Training the model\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Validating the model by predicting rating for X_test\n",
    "# Saving the values into y_pred\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1.8 it is shown that rating is not continues, so to increase the accuracy the values are rounded to the multipes of 0.5. It appears it has a very positive impact on MAE, reducing it by 0.02 or so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_rating_pred(rating_pred):\n",
    "    '''INPUT: a value of the rating\n",
    "    OUTPUT: a value \\'rounded\\' to a multiple of 0.5'''\n",
    "    if rating_pred <= 0.5:\n",
    "        return 0.0\n",
    "    if rating_pred <= 1.5:\n",
    "        return 1.0\n",
    "    if rating_pred <= 1.75:\n",
    "        return 1.5\n",
    "    if rating_pred <= 2.25:\n",
    "        return 2.0\n",
    "    if rating_pred <= 2.75:\n",
    "        return 2.5\n",
    "    if rating_pred <= 3.25:\n",
    "        return 3.0\n",
    "    if rating_pred <= 3.75:\n",
    "        return 3.5\n",
    "    if rating_pred <= 4.25:\n",
    "        return 4.0\n",
    "    if rating_pred <= 4.75:\n",
    "        return 4.5\n",
    "    return 5.0\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    y_pred[i] = round_rating_pred(y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares the y_pred and y_test, showing the mean absolute error\n",
    "# between the two values\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows which values have the most impact on the prediction\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "feat_importances = pd.Series(regr.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Applying the model and submitting the result\n",
    "Repeating the process done with the training DF and submitting the result#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df.loc[df['sample'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop('rating', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(to_drop, axis=1)\n",
    "final_df = final_df.drop(negative_impact, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.merge(city_dummies, left_index=True, right_index=True)\n",
    "final_df = final_df.merge(cuisine_dummies, left_index=True, right_index=True)\n",
    "final_df = final_df.merge(prices_dummies, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_submission = regr.predict(final_df)\n",
    "for i in range(len(predict_submission)):\n",
    "    predict_submission[i] = round_rating_pred(predict_submission[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['Rating'] = predict_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of my work\n",
    "## General look\n",
    "There were plenty of expected and unexpected results. Feature Engineering provided to be the hardest aspect of this task, as well as I discovered some interesting methods to working with given data. Certain things came up from random mistakes(e.g. keeping both the ranking and normalised ranking columns).\n",
    "## Potential Improvements\n",
    "Over the course of the task, I didn't manage to use the entirety of the data given - I wish I could use the most popular cuisine per city, or make a simple analysis of the reviews, yet I did not have enough time. I would also like to improve visual representation of the code, making it more structural, and potentially adding classes, but I was not confident enough in my own ability to work with them, so I decided to stick to the conventional code and convetional functions. The code could also be improved by adding a few extra functions, and possibly the acuracy of the model can be improved by dropping some columns that have little to no impact on the MAE. Decreacing the boundary for correlations might also increase the accuracy of the model.\n",
    "## Things I Liked About My Model\n",
    "My initial goal was to break the barrier of 0.2, which I managed to do, ending up with MAE of 0.1753. One of the most unpected things was the effect rounding had on MAE, changing it by 0.02, which was one of the biggest jumps in this value. I also did not expect the status of a capital to have any effect on MAE, but apparently it was more important than I initially expected. During one of the tests, I forgot to drop the ranking column, and I figured out that ranking and normalised ranking both helped the model to increase the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
